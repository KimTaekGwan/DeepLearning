{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-pptx\n",
    "# !pip install transformers\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !conda install -c pytorch pytorch\n",
    "# !conda install -c conda-forge tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ktg/Desktop/DeepLearning/Simila/data/original/deep_learning_intro.pptx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir_path = \"/Users/ktg/Desktop/DeepLearning/Simila/data/original\"\n",
    "\n",
    "for (root, directories, files) in os.walk(dir_path):\n",
    "    for file in files:\n",
    "        # if '.txt' in file:\n",
    "        file_path = os.path.join(root, file)\n",
    "        print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Text_Preprocessing:\n",
    "    def __init__(self) -> None:\n",
    "        self.punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "        self.mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \n",
    "                                \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \n",
    "                                \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n",
    "                                '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', \n",
    "                                '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', } \n",
    "        self.specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "\n",
    "    def preprocessing(self, text):\n",
    "        text = self.clean(text)\n",
    "        text = self.clean_str(text)\n",
    "        text = self.token(text)\n",
    "        return text\n",
    "    \n",
    "    def clean(self, text):\n",
    "        for p in self.mapping:\n",
    "            text = text.replace(p, self.mapping[p])\n",
    "        \n",
    "        for p in self.punct:\n",
    "            text = text.replace(p, f' {p} ')\n",
    "        \n",
    "        \n",
    "        for s in self.specials:\n",
    "            text = text.replace(s, self.specials[s])\n",
    "        return text.strip()\n",
    "\n",
    "    def clean_str(self, text):\n",
    "        # pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "        # text = re.sub(pattern=pattern, repl='', string=text)\n",
    "        pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "        text = re.sub(pattern=pattern, repl='', string=text)\n",
    "        pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "        text = re.sub(pattern=pattern, repl='', string=text)\n",
    "        pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "        text = re.sub(pattern=pattern, repl='', string=text)\n",
    "        pattern = '[^\\w\\s\\n]'         # 특수기호제거\n",
    "        text = re.sub(pattern=pattern, repl='', string=text)\n",
    "        text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]',' ', string=text)\n",
    "        text = re.sub('\\n', ' ', string=text)\n",
    "        return text \n",
    "    \n",
    "    def token(self, text) -> list:\n",
    "        ls = [t.strip() for t in text.split(' ') if t]\n",
    "        res = ' '.join(ls)\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPT_Info_Extract:\n",
    "    def __init__(self, path) -> None:\n",
    "        path = \"data/original/deep_learning_intro.pptx\"\n",
    "        self.parsed = Presentation(path)\n",
    "        self.text_dict = {'total':[], 'page':defaultdict(list)}\n",
    "        self.link_list = []\n",
    "        self.img_dict = {}\n",
    "        self.text_info = None\n",
    "        self.page_num = None\n",
    "        self.img_num = 0\n",
    "        self.tp = Text_Preprocessing()\n",
    "        self.image_blob = []\n",
    "        \n",
    "    def setting(self):\n",
    "        pass\n",
    "    \n",
    "    def last(self):\n",
    "        for text_ls in self.text_dict['page'].values():\n",
    "            for text in text_ls:\n",
    "                # print(text)\n",
    "                pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$\\-@\\.&+:/?=]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "                links = re.findall(pattern, text)\n",
    "                if links:\n",
    "                    for link in links:\n",
    "                        self.link_list.append(link)\n",
    "                    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "                \n",
    "                if text:\n",
    "                    self.text_dict['total'].append(text)\n",
    "        \n",
    "        self.text_info = self.tp.preprocessing(' '.join(self.text_dict['total']))\n",
    "    \n",
    "    def extract(self):\n",
    "        for idx, slide in enumerate(self.parsed.slides):\n",
    "            self.page_num = idx+1\n",
    "            for shape in slide.shapes:\n",
    "                self.group_check(shape)\n",
    "        \n",
    "        self.last()\n",
    "    \n",
    "    def group_check(self, shape):\n",
    "        # 그룹 모형\n",
    "        if shape.shape_type == MSO_SHAPE_TYPE.GROUP:\n",
    "            for s in shape.shapes:\n",
    "                self.info_extract(s)\n",
    "        # 그룹 모형 아님\n",
    "        else:\n",
    "            self.info_extract(shape)\n",
    "    \n",
    "    def info_extract(self, shape):\n",
    "        if shape.shape_type == MSO_SHAPE_TYPE.GROUP:\n",
    "            self.group_check(shape)\n",
    "        else:\n",
    "            # yes image\n",
    "            if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:\n",
    "                image_blob = shape.image.blob\n",
    "                if image_blob not in self.image_blob:\n",
    "                    ext = shape.image.ext # - (확장명)\n",
    "                    size = shape.image.size\n",
    "                    if ext in ['png', 'jpeg', 'jpg'] and size[0] >= 200 and size[1] >= 200:\n",
    "                        self.img_num += 1\n",
    "                        num = str(self.img_num).zfill(3)\n",
    "                        save_path =f\"res/image_{num}.{ext}\"\n",
    "                        with open(save_path, \"wb\") as file:\n",
    "                            file.write(image_blob)\n",
    "                        self.image_blob.append(image_blob)\n",
    "                        self.img_dict[self.img_num] = save_path\n",
    "            # no image\n",
    "            else:\n",
    "                # yes text\n",
    "                if shape.has_text_frame:\n",
    "                    if shape.text.strip() != \"\":\n",
    "                        self.text_dict['page'][self.page_num].append(shape.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt = PPT_Info_Extract('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/nlpconnect/vit-gpt2-image-captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ppt_3_7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/ppt_3_7/lib/python3.7/site-packages/transformers/models/vit/feature_extraction_vit.py:31: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in tqdm(image_paths):\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:00<00:00, 136.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a blue and white photo of a person holding a remote control',\n",
       " 'a close up picture of a clock on a wall',\n",
       " 'an aerial view of a piece of artwork on a piece of paper',\n",
       " 'a painting of a person holding a toy elephant',\n",
       " 'a series of photographs showing a clock on a wall',\n",
       " 'an aerial view of a clock on a wall',\n",
       " 'a collage of photos showing different types of electronic devices',\n",
       " 'a series of images showing a person holding a blue and white disk',\n",
       " 'a collage of photos showing different types of electronic devices',\n",
       " 'a collage of photos of a person holding an electronic device',\n",
       " 'a black and white photo of a clock on a wall',\n",
       " 'a small kitten sitting on the floor in front of a mirror',\n",
       " 'a white dog is looking at the camera',\n",
       " 'a gray and white cat sitting on top of a sandy surface',\n",
       " 'a brown and white dog laying on the floor',\n",
       " 'a cat sitting on the floor looking at the camera',\n",
       " 'a wooden table topped with lots of different colored balls',\n",
       " 'a blue and white photo of a person on a blue and white tv',\n",
       " 'a drawing of a man with a clock on his head',\n",
       " 'a computer generated image of a person on a cell phone',\n",
       " 'a photo taken from inside of a cell phone',\n",
       " 'a series of photos showing different types of kites',\n",
       " \"a series of photographs showing a person's handwriting\",\n",
       " 'a series of photographs of a group of airplanes',\n",
       " 'a computer generated image of a person holding a remote control',\n",
       " 'a row of colorful kites on a blue background',\n",
       " 'a collage of photos of a street sign and a building',\n",
       " 'a collage of different types of objects on a table',\n",
       " 'a collage of photographs of a group of black and white cats',\n",
       " 'a collage of photos of a cat and a dog',\n",
       " 'a sign that is on the side of a building',\n",
       " 'a shot of a sunset with a penguin in the distance',\n",
       " 'a blurry picture of a person holding a fire extinguisher',\n",
       " 'a teddy bear sitting on top of a white background',\n",
       " 'a collage of photos of a person holding a cell phone',\n",
       " 'a series of photos showing different types of lights',\n",
       " 'a collage of photos of a dog sitting on a bench',\n",
       " 'a collage of images showing a person on a cell phone']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(list(ppt.img_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Pillow\n",
    "# pip install pdfminer.six\n",
    "# pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(\"test.pdf\")\n",
    "\n",
    "# Iterate through each page of the PDF\n",
    "for page_number in range(pdf_document.page_count):\n",
    "    page = pdf_document[page_number]\n",
    "    \n",
    "    # Get the page's size\n",
    "    page_width = page.bound().width\n",
    "    page_height = page.bound().height\n",
    "    \n",
    "    # Create a pixmap with the same size as the page\n",
    "    pixmap = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n",
    "    \n",
    "    # Save the pixmap as a PNG file\n",
    "    png_filename = f\"page_{page_number + 1}.png\"\n",
    "    pixmap.save(png_filename)\n",
    "    \n",
    "    # Free up memory\n",
    "    pixmap = None\n",
    "\n",
    "# Close the PDF file\n",
    "pdf_document.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 images found on page 0\n",
      "No images found on page 1\n",
      "2 images found on page 2\n",
      "2 images found on page 3\n",
      "2 images found on page 4\n",
      "No images found on page 5\n",
      "1 images found on page 6\n",
      "No images found on page 7\n",
      "5 images found on page 8\n",
      "2 images found on page 9\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "file_path = \"test3.pdf\"\n",
    "open_file = fitz.open(file_path)\n",
    "\n",
    "for page_number in range(len(open_file)):\n",
    "    page = open_file[page_number]\n",
    "    list_image = page.get_images()\n",
    "\n",
    "    if list_image:\n",
    "        print(f\"{len(list_image)} images found on page {page_number}\")\n",
    "        \n",
    "        for i, image in enumerate(list_image):\n",
    "            # Get the pixmap of the image\n",
    "            pixmap = fitz.Pixmap(open_file, image[0])\n",
    "            \n",
    "            # Save each image as a separate file\n",
    "            image_filename = f\"page_{page_number + 1}_image_{i + 1}.png\"\n",
    "            pixmap.save(image_filename)\n",
    "    else:\n",
    "        print(\"No images found on page\", page_number)\n",
    "\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.dbpia.co.kr\n",
      "\n",
      "정지수외6인: 문서유사도를통한관련문서분류시스템연구   77(Jisoo Jeong et al.: Related Documents Classification System by Similarity between Documents)Copyright Ⓒ2016 Korean Institute of Broadcast and Media Engineers. All rights reserved.“This is an Open-Access article distributed under the terms of the Creative Commons BY-NC-ND (http://creativecommons.org/licenses/by-nc-nd/3.0) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited and not altered.”a)세종대학교소프트웨어융합학과(Department of Software Convergence, Sejong University)b)세종대학교디지털콘텐츠학과(Department of Digital Contents, Sejong University)c)세종대학교인공지능언어공학과(Department of Artificial  Intelligence and Linguistic Engineering, Sejong University) d)세종대학교소프트웨어학과(Department of Software, Sejong University)‡Corresponding Author :김원일(Wonil Kim)E-mail: wikim@sejong.ac.krTel: +82-3408-3795ORCID: https://orcid.org/0000-0002-1489-8427※This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education(2015R1D1A1A01060693)・Manuscript received November 16, 2018; Revised December 27, 2018; Accepted January 10, 2019.문서유사도를통한관련문서분류시스템연구정지수a), 지민규a), 고명현b), 김학동b), 임헌영b), 이유림c), 김원일d)‡Related Documents Classification System by Similarity between DocumentsJisoo Jeonga), Minkyu Jeea), Myunghyun Gob), Hakdong Kimb), Heonyeong Limb),        Yurim Leec), and Wonil Kimd)‡요약본논문은머신러닝기술을이용하여과거의수집된문서를분석하고이를바탕으로문서를분류하는방법을제안한다. 특정도메인과관련된키워드를기반으로데이터를수집하고, 특수문자와같은불용어를제거한다. 그리고한글형태소분석기를사용하여수집한문서의각단어에명사, 동사, 형용사와같은품사를태깅한다. 문서를벡터로변환하는Doc2Vec 모델을이용해문서를임베딩한다. 임베딩모델을통하여문서간유사도를측정하고머신러닝기술을이용하여문서분류기를학습한다. 학습한분류모델간성능을비교하였다. 실험결과, 서포트벡터머신의성능이가장우수했으며F1 점수는0.83이도출되었다.AbstractThis paper proposes using machine-learning technology to analyze and classify historical collected documents based on them. Data is collected based on keywords associated with a specific domain and the non-conceptuals such as special characters are removed. Then, tag each word of the document collected using a Korean-language morpheme analyzer with its nouns, verbs, and sentences. Embedded documents using Doc2Vec model that converts documents into vectors. Measure the similarity between documents through the embedded model and learn the document classifier using the machine running algorithm. The highest performance support vector machine measured 0.83 of F1-score as a result of comparing the classification model learned.Keyword : Document analysis, Related document, Doc2Vec, Machine learning, Document classification특집논문(Special Paper)방송공학회논문지제24권제1호, 2019년1월(JBE Vol. 24, No. 1, January 2019)https://doi.org/10.5909/JBE.2019.24.1.77ISSN 2287-9137 (Online) ISSN 1226-7953 (Print)\fwww.dbpia.co.kr\n",
      "\n",
      "78방송공학회논문지제24권제1호,2019년1월(JBE Vol. 24, No. 1, January 2019)Ⅰ. 서론정보사회에서문서의수는기하급수적으로늘어나고있다. 과거의문서들은현재사건들의근거가되기때문에웹상에서문서들을수집하고분석하는일은매우중요하다. 이러한작업을하기위해텍스트데이터를다루는일이필요하다[1]. 텍스트마이닝(text mining)은비구조적인텍스트문서로부터정보를찾아지식을발견하는것으로텍스트분류는텍스트마이닝연구분야의부분이다[2][3]. 목적에따라정보를추출하기위해서는분석을통해분류를할필요가있다. 문서분류를위해서는텍스트데이터를정형데이터로변환해야하고이러한연구에는통계적정보기반, 신경망기반방법등이존재한다. 정형데이터로변환하고문서를분류하는과정에서다양한방법이제시된다. 텍스트데이터를정형화하는방법으로벡터값으로표현하는방법이일반적이며, 분류의성능을높이기위해서연구과정이진행되고있다. 본연구에서는단어를벡터로표현하여단어간의거리로단어의의미와유사도를알수있는단어임베딩에대해주목하였다. 단어만이아닌문장, 단락, 문서와같은길이의텍스트를벡터로표현하고, 문서간의유사도를측정할수있는문서임베딩모델에주목하였고, 해당모델을통한분류과정이미흡하다판단하여직접데이터수집을위한웹크롤러를설계하고데이터의일부를분류목적에따른전처리과정을진행하였다. 정제된데이터를문서임베딩을위한모델로써학습한다. 학습된모델을토대로분류알고리즘으로가장뛰어난성능을보인4가지의머신러닝기술을사용하여각각성능을비교분석한다.  본연구는문서유사도를통한임베딩모델을토대로분류기에주로사용되는4개의머신러닝알고리즘을비교분석한다. 웹크롤러를통해서정해진카테고리내의분류문서들이아닌, 수집할특정도메인과관련된키워드를필터링하여과거문서들을수집한다. 텍스트데이터를벡터로표현하는Doc2Vec 모델에대하여연구하고이를통해수집한문서를임베딩한다. 그리고머신러닝기술을사용해서문서의주제를분류하는방법을연구한다.본논문의구조는다음과같다. 제2장에서이전연구를, 제3장에서본논문에서제안하는문서분석을통한관련문서분류시스템에대한설계를소개한다. 제4장에서는제안모델을이용한실험을진행하고결과를기술하였다. 제5장에서는본연구의결론및향후연구방향에대해논의한다.Ⅱ. 이전연구1. TF-IDF자연어처리기술에서비정형텍스트데이터를정형화하는것은텍스트데이터를다루기위한가장기본적이며중요한과정이다. 정형화를위해TF-IDF(Term Frequency- Inverse Document Frequency)와같은통계적방법을이용할수있다. TF는특정단어가문서내에서등장하는빈도를토대로정해지는가중치가적용된통계적수치이다. IDF는전체문서군크기에서특정단어가나타난문서수를나누어서구할수있다. TF-IDF는TF와IDF를곱한값으로해당단어가문서의특징을구분하는지평가하며, 범용적으로널리사용되고있는통계적기법이다. 이전연구에서TF-IDF를이용하여키워드추출을통해추천시스템에응용하였다. 문서분류에서핵심적인키워드추출을TF-IDF를이용해TF-IDF값이높은단어일때추출된키워드가문서분류를하는것에있어중요하다. 추출된키워드가곧문서분류의라벨과연관이있으며이러한TF-IDF방식은문서분류에있어좋은결과를나타낸다[4]. 국외의이전연구[5]에서웹문서의검색결과성능을향상시키기위해주제어를추출하여유사도에따라문서를분류하였다. 문서분류에있어TF-IDF는문서의내용을대표하는주제어를추출하는데유용하게사용되어왔지만빈도정보에의한방법은한계가있다. 따라서본논문은빈도정보뿐만아니라단어의의미를알수있는임베딩모델에주목하였다. 문서의내용에대한유사도를측정하는임베딩모델을학습한후, 분류모델의비교분석을하여성능향상에기여한다.  2. Word2Vec단어를벡터로표현하는과정을단어임베딩(Word em-bedding)이라고부르며, 단어임베딩을위해서는벡터공간모델(Vector Space Model)이주로사용된다[6]. 벡터공간모\fwww.dbpia.co.kr\n",
      "\n",
      "정지수외6인: 문서유사도를통한관련문서분류시스템연구   79(Jisoo Jeong et al.: Related Documents Classification System by Similarity between Documents)그림2. 문서분류기의전체적인설계FIg. 2. Overall Design of Document Classification그림1. 벡터거리로단어간의의미파악FIg. 1. Understand the meaning between words in vector distance델은벡터공간내주변에등장하는단어는서로비슷한의미를가진다는분산가설(Distributional Hypothesis)(Harris, 1954)에기반을둔다. 의미상유사한단어를서로인접시켜임베딩시키므로벡터로표현된단어들은문법적인부분만아니라의미적인부분까지반영된다. 결론적으로, 단어임베딩을통해생성된벡터에따르면거리가가까울수록단어들은서로비슷한벡터를가지게된다. 또한단어벡터에포함된단어들은모두수치화되어있기때문에단어와단어간의거리를활용한벡터연산이가능하고이로인해추론을할수있게된다[7].그림1은단어간의거리를통해유사도를알수있음을보여준다. 벡터화하여단어간의거리를계산하여측정, 추론할수있다면단어간의의미를알수있고, 이러한방법으로정형화을하는것을Word2Vec이라고한다.  Word2Vec을통해학습한모델로분류기를만든이전연구[8]들이존재하고, 단어임베딩모델을통한분류기는좋은성능을보인다. 단어만을벡터로표현하는것이아닌문장, 단락, 문서와같은가변길이의텍스트를벡터로표현하기위해확장형인Doc2Vec 모델이제안되었고, 해당방식은문서분류, 감정분석, 정보검색에좋은성능을보인다[9].본연구에서는데이터수집을위해관련키워드를필터링하여특정도메인에관한문서를수집하기위한웹크롤러를만든다. 수집한일부데이터를학습데이터로사용한다. 워드임베딩방식의하나인Doc2Vec 모델의품질을높이기위해전처리과정을진행한후, 모델을학습한다. 기존분류기머신러닝기술을사용하여분류모델평가를나타낸다. 실험한결과로비교분석하고, 개선사항및향후연구방향에대해논의한다.Ⅲ. 제안방법본연구에서제안하는문서분류시스템의전체구조는그림2와같다. 웹크롤러를이용하여특정도메인에관한\fwww.dbpia.co.kr\n",
      "\n",
      "80방송공학회논문지제24권제1호,2019년1월(JBE Vol. 24, No. 1, January 2019)그림3. 문서분류기의데이터전처리과정Fig. 3. Document calssification data preprocessing문서를수집하고데이터베이스에저장한다. 그림3은저장한문서에 대하여전처리를진행하고문서를벡터로임베딩하는Doc2Vec모델을학습한다. 그리고머신러닝기술을이용하여문서분류기를학습한다.1. 웹크롤러설계문서분류기에사용할학습용데이터를수집하기위해웹크롤러를설계한다. 현재인터넷에는매우방대한양의문서데이터들이존재하므로모든문서데이터를수집하기어렵다. 따라서분류에필요한데이터를가져옴과동시에데이터의양을줄이기위해특정도메인을선정하고해당도메인에대한문서를수집한다. 특정도메인과관련된뉴스기사를선정하고, 관련된키워드(Keyword)를중심으로포함여부에따라필터링하는방식으로웹크롤러를설계하였다. 그림4의웹크롤러는특정도메인에관한문서를수집하기위해키워드를기반으로필터링작업을거친다. 수집한그림4. 키워드를기반으로수집하는웹크롤러설계Fig. 4. Web crawler designs collected based on keywords 데이터는HTML 태그를제거하고, 필요부분만추출하는전처리과정을거쳐뉴스의제목(Title)과뉴스의본문(Con- tent)을데이터베이스에저장한다.2. 데이터전처리데이터전처리과정은데이터가결측치(none value), 이상값(outlier), 에러(error), 잡음(noise) 등을포함하여발생하는문제를해결하기위한필수적인과정이며데이터전처리를통해모델의성능향상에기여한다. 본연구에서는데이터전처리과정을통해특수문자와같은불필요한문자들을제거하고, 오래된뉴스인경우본문내용에한자가있어한자를음독으로치환한다. 기사내용의불필요한기자이메일주소와영어단어및문장을제거한다. 다음으로한글형태소분석및품사태깅을지원하는라이브러리KoNLPy의Komoran클래스를사용하여형태소분석을실시한다[10]. 형태소분석을통해동일한단어라도품사를결합하면의미를구분할수있기때문에모델의성능향상에기여한다.3. Doc2vec 모델학습텍스트데이터를머신러닝기술에적용하기위해서문서를벡터값으로변환해야한다. 단어의순서와의미는중요한정보로, 이를표현하기위해단어를벡터화하는Word2Vec 모델의확장형인Doc2Vec 모델을사용한다. 단\fwww.dbpia.co.kr\n",
      "\n",
      "정지수외6인: 문서유사도를통한관련문서분류시스템연구   81(Jisoo Jeong et al.: Related Documents Classification System by Similarity between Documents)그림5. Doc2Vec 모델구조(PV-DM)Fig. 5. Doc2Vec Model structure (PV-DM)그림6. 머신러닝기술분류기실험Fig. 6. Machine Learning Algorithm Classifier Experiment어만이아닌단어들로이루어진문장, 단락, 문서와같은가변길이의텍스트를벡터로표현하기위한방법으로Doc2Vec이제안되었다. Doc2Vec은문장, 단락, 문서와같은길이의텍스트를임베딩하는모델로써, Word2Vec 알고리즘을문장, 단락또는전체문서와같이더큰텍스트블록에대한연속표현을비지도학습하도록수정된모델이다. Doc2Vec은모델을훈련할때라벨과실제데이터가필요한데, 이때의라벨은문서의주제, 빠진내용이사용되거나일반적으로문서의파일명, 문서번호, 혹은본연구에서사용할2가지의문서분류라벨명으로사용된다. 문서간의유사도를측정하여문서분류의성능을높이고라벨의정의된데이터를정확히분류하여출력결과를보인다. Doc2Vec은문서 분류, 감정분석, 정보검색에적합하나, 실질적으로문서분류에사용한경우가TF-IDF에비해적어본연구에서제안하는Doc2Vec 모델을이용하여문서를임베딩하는모델을학습한후, 관련문서를분류하고성능을평가한다. 문서단어의빈도수를가중치로사용한TF- IDF방식과는달리문서를벡터로표현하고유사도를통해문서분류하는방식에주목하였다.그림5는Doc2Vec 모델학습을위한방법으로DBOW (Distributed Bag Of Words) 방식과PV-DM(Distributed Memory Model of Paragraph Vector) 방식으로2가지학습방식중, 문서벡터와단어벡터정보를모두사용하는PV- DM 모델방식을이용하였다[11]. 그림5는PV-DM 모델의구조를나타낸다. d는문서를, w는단어를의미한다. N은벡터의크기를나타내고, V는전체단어의수를의미한다. \fwww.dbpia.co.kr\n",
      "\n",
      "82방송공학회논문지제24권제1호,2019년1월(JBE Vol. 24, No. 1, January 2019)News classnumbercontentEvent news1반영구화장용문신염료에서발암물질과중금속이다량검출돼주의가필요하다. 유해물질중니켈은피부알레르기를잘유발시키는대표적인금속물질이다.(Because a large amount of carcinogens and heavy metals are detected in the dye for cosmetics of semitransparent cells, care needs to be taken. Nickel is a representative metal substance that causes skin allergies well.)Other news0그린을팔지않으면장사가안된다는환경시대를맞아녹황색채소수세미은행잎등식물을원료로쓴자연화장품이잇따라선보이고있다.(In the wake of the environmental era when green is not sold, natural cosmetics such as green vegetables, susemi, and ginkgo leaves are showing off one after another.)표1. 위해도관련뉴스와그외의뉴스의라벨링된데이터예시Table 1. Examples of hazad related News and other news labeled dataInput에서는단어를one-hot encoding으로넣어주고, 주변의단어w(t-2, t-1,t+1, t+2)를각각projection 시킨후그벡터들의평균을구해서Hidden에보낸다. 다음으로여기에Weight Matrix를곱해서Output으로보내고softmax 계산을한다. 계산한결과를타깃단어인w(t)의one-hot en-coding과비교하여에러를계산한다. 이때의d는메모리와같은기능으로현재의문맥에서빠진것이나단락에서의주제를기억해주는역할을한다. 본연구에서d는특정주제로분류된문서들과그외의문서범주를의미한다.4. 분류기설계Doc2Vec 모델을학습한후, 학습된모델에서추출한문서벡터로각각의뉴스데이터들을관련문서로분류할수있도록4개의머신러닝기술을사용한다. 각각의머신러닝기술은분류모델에있어좋은결과를나타내는머신러닝기술로, 학습된Doc2Vec 모델을토대로머신러닝기술을적용하여분류모델의실험에사용하였다.그림6은학습된Doc2Vec 모델을토대로사용한머신러닝기술의실험과정을나타낸다. 첫번째로반응변수가1 또는0인이진형변수에서쓰이는분류방법의일종인로지스틱회귀분석(Logistic Regression)[12]이다. 두번째로데이터를분석하여이들사이에존재하는패턴을예측가능한규칙들의조합으로나타내는방법인결정트리(Decision Tree)[13], 세번째로주어진데이터점들이두개의클래스안에각각속해있다고가정했을때, 새로운데이터점이두클래스중어느곳에속하는지결정하는것이목표로하는분류방법인서포트벡터머신(Support Vector Machine)[14]이다. 마지막으로사건B가발생한경우A의확률을나타내는베이스정리(Bayesian probability)를기반으로하는분류방법인나이브베이스(Naive Bayesian)[15]이다. 이4개의머신러닝기술을사용한분류모델을비교분석한다. 본연구에서는특정도메인의뉴스기사를분류하고, 분류의목적으로뉴스기사의내용이위해한주제의뉴스인지, 위해하지않은지에대한뉴스를분류하기위해2가지의라벨을통해학습데이터를구축한후모델을비교평가한다.Ⅳ. 실험결과및논의1. 실험데이터학습데이터를만들기위해본연구에서는연구에사용하기위한데이터수집을할필요가있다. 실험에사용하기위한데이터를공개적으로배포되지않으므로특정도메인에관한데이터를수집하기위한웹크롤러를통해진행하였다. 연구주제인“생활화학제품”에관련된품목과위해관련키워드를필터링과정을통해뉴스기사를수집하였다. 수집한4만건의데이터중6000개의데이터를학습데이터로써사용하였다. 그후수작업을통해라벨링과정을진행하여위해도관련뉴스와관련없는뉴스데이터로분류하였다.표1에서News class에서Event news는위해도관련뉴스를의미하고, 해당라벨링은‘1’으로지정하고, content에서는라벨링된뉴스데이터의내용을의미한다. Other news는그외의관련되지않은뉴스를나타내고해당라벨링은‘0’으로표기하고, content에서는라벨링된해당뉴스의내용을의미한다. 실험을위하여4200개를훈련용데이\fwww.dbpia.co.kr\n",
      "\n",
      "정지수외6인: 문서유사도를통한관련문서분류시스템연구   83(Jisoo Jeong et al.: Related Documents Classification System by Similarity between Documents)ModelAccuracyPrecisionRecallF1-scoreLogistic Regression0.920.850.800.82Decision Tree0.830.900.780.81(Gaussian) Naive Bayesian0.910.870.780.80Support Vector Machine0.880.850.820.83표3. 분류모델성능비교분석Table 3. Classification Model Performance Comparison Analysis터로, 1800개를검증용데이터로분류하였다. 본연구에서는“생활화학제품”에대해위해한뉴스와그렇지않은뉴스로분류하기위해2개의라벨링과정을진행하였다.2. 실험과정실험데이터를전처리과정을통해불필요한텍스트를제거한후, 비정형데이터를정형화데이터로변환한다. 라이브러리gensim[16]에서제공하는Doc2Vec 모델을사용하였고, 해당Parameter에서다음과같이설정하였다.OptionValueDM1Vector_size100alpha0.025min_alpha0.025epoch100표2. Doc2Vec ParameterTable 2. Doc2Vec Parameter  표2의옵션에서DM을1로지정한것은해당모델을사용함을의미한다, Vector_size는벡터의크기(dimension), alpha는사용자가지정하는학습률(learning rate)를의미하고, min_alpha는최소학습률을의미한다. epoch는모델의훈련횟수를나타낸다. Doc2Vec 모델Parameter에서epoch를100 설정한만큼학습된DM모델이만들어진다. 그후4개의머신러닝기술을각각적용하여분류모델을만든다.3. 실험결과4개의머신러닝기술을사용하여분류기를각각만든후, 모델성능평가지표로비교분석한다. 분류모델의성능평가지표는크게4가지로평가된다. 정확도(Accuracy)와정밀도(Precision), 재현율(Recall), 그리고정밀도와재현율의조화평균값인F1 점수(F1-score)이다. (1)수식1에서TP(True Positive)는위해도관련뉴스인것을제대로검출된것을, TN(True Negative) 그외의뉴스를제대로검출된것이며, FP(False Positive)는위해도뉴스임에도그외의뉴스로잘못검출된것으로, FN(False Nega- tive)는그외의뉴스임에도위해도뉴스로잘못검출된것임을의미한다. 수식1에서표기된식으로4개의분류모델의성능평가지표를비교분석하였다.표3 실험결과로정확도의경우로지스틱회귀분석이0.92로가장높은정확도가나왔고, 정밀도의경우결정트리가0.90으로가장높은정밀도를나타냈다. 재현율과F1 점수의경우서포트벡터머신이각각0.82, 0.83으로분류모델의성능평가에서F1 점수가가장높은점수가성능이좋은모델로써평가하기에4개의머신러닝기술중서포트벡터머신이가장높은성능을나타냈다. Ⅴ. 결론본연구는머신러닝기술을이용하여과거의수집된문\fwww.dbpia.co.kr\n",
      "\n",
      "84방송공학회논문지제24권제1호,2019년1월(JBE Vol. 24, No. 1, January 2019)서를분석하고이를바탕으로문서분류모델을제안한다. 연구를위해도메인과관련된키워드를기반으로데이터를수집하고, 특수문자와같은불용어를제거한다. 그리고형태소분석기를사용하여수집한문서의각단어에품사를태깅한다. 문서를벡터로변환하는Doc2Vec 모델을이용해임베딩모델을만든후문서간유사도를측정하고머신러닝기술을이용하여문서분류기를학습하였다. 연구에사용할생활화학제품관련4800개의뉴스데이터를Doc2Vec 모델로학습시킨후4개의머신러닝기술을사용하여분류기성능을비교실험하였다. 실험결과, 서포트벡터머신의F1 점수가0.83으로가장좋은성능을보였다.향후에는본연구결과를기반으로제품에관련된내용과위해정보의포함여부에대해라벨링하여진행한후학습데이터를만든다. 그후, 다양한분야에적용할수있는다중분류모델및데이터의계층적표현을위한분류모델을연구한다.참고문헌(References) [1] Jun-Ho Roh, Han-joon Kim, Jae-Young Chang. \"Improving Hypertext Classification Systems through WordNet-based Feature Abstraction.\" The Jounal of Society for e-Business Studies, 18.2 pp.95-110(6) 2013.May[2] YunJeong Choi, SeungSoo Park. \"Interplay of Text Mining and Data Mining for Classifying Web Contents.\" KOREAN JOURNAL OF COGNITIVE SCIENCE, 13.3 pp.33-46.(14) 2002.9[3] Sunghae Jun \"A Big Data Preprocessing using Statistical Text Mining\" Journal of Korean Institute of Intelligent Systems Vol. 25, No. 5, pp. 470-476(7) 2015 October[4] Eun-Soon You, Gun-Hee, Choi, Seung-Hoon Kim \"Study on Extraction of Keywords Using TF-IDF and Text Structure of Novels\"  Korean Society of Computer Information Volume 20, Issue 2, pp.121-129(9)    2015 February [5] J. Ramos, “Using tf-idf to determine word relevance in document quer-ies”, In Proceedings of the First Instructional Conference on Machine Learning, 2003[6] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean \"Distributed Representations of Words and Phrases and their Compositionality\" NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 pp.3111-3119(9) Lake Tahoe, Nevada  December 2013 [7] Garam Choi, Sung-Pil Choi \"A Study on the Deduction of Social Issues Applying Word Embedding: With an Empasis on News Articles related to the Disables\" Journal of the Korean Society for Information Management, 35(1)  pp.231-250 (20) 2018.3[8] Jung-Mi Kim, Ju-Hong Lee. \"Text Document Classification Based on Recurrent Neural Network Using Word2vec.\" Journal of Korean Institute of Intelligent Systems, 27.6  pp. 560-565 (6) 2017.12[9] Quoc Le ,Tomas Mikolov \"Distributed Representations of Sentences and Documents\" ICML'14 Proceedings of the 31st International Conference on International Conference on Machine Learning Volume 32 pp.1188-1196(9)  Beijing, China  June 2014[10] Lucy Park, Sungzoon Cho, “KoNLPy : Korean natural language proc-essing in Python” Proceeding soft he 26th Annual Conferenceon Human & Cognitive Language Technology, 2014 10[11] Seong-Ho Choi, Eun-Sol Kim, Byoung-Tak Zhang \"An Intention Prediction Method for Dialogue using Paragraph Vector\" Korea Computer Congress 2016 pp.977-979(3) 2016.6[12] KyuWan Kim, HyunJu Shin, SunJin Kim, KyoungDuek Moon, HyunAh Lee. \"Detecting Improper Paragraphs in a News Article Using Logistic Regression Classification and Inter-class Similarity.\" Journal of Computing Science and Engineering  pp.1873-1875.(3) 2017.12[13] Dan-Ho Park, Won-Sik Choi, Hong-Jo Kim, Seok-Lyong Lee. \"Web Document Classification System Using the Text Analysis and Decision Tree Model.\" Journal of Computing Science and Engineering, 38.2A 248-251.(4) 2011.11[14] Do-Sik Min, Mu-Hee Song, Ki-Jun Son, Sang-Jo Lee. \"Spam - mail Filtering Using SVM Classifier.\" Journal of Computing Science and Engineering 30.1B pp.552-554.(3) 2003.4[15] Song-yi Han, Yong-Gyu Jung. \"Spam Filtering Using A Complement Naive Bayesian Classifier.\"  Journal of Computing Science and Engineering, 36.2C 325-328.(4) 2009.11[16] scikit-learn, https://scikit-learn.org/stable/\fwww.dbpia.co.kr\n",
      "\n",
      "정지수외6인: 문서유사도를통한관련문서분류시스템연구   85(Jisoo Jeong et al.: Related Documents Classification System by Similarity between Documents)저자소개정지수- 2018년: 숭실대학교평생교육원컴퓨터공학학사- 2018년~ 현재: 세종대학교소프트웨어융합학과석사과정- ORCID : https://orcid.org/0000-0003-3756-1074- 주관심분야: 텍스트마이닝, 기계학습, 딥러닝지민규- 2018년: 세종대학교천문우주학과학사- 2018년~ 현재: 세종대학교소프트웨어융합학과석사과정- ORCID : https://orcid.org/0000-0002-3089-1452- 주관심분야: 텍스트마이닝, 기계학습, 딥러닝고명현- 2016년: 세종대학교디지털콘텐츠학과학사- 2016년~ 현재: 세종대학교디지털콘텐츠학과석사과정- ORCID : https://orcid.org/0000-0002-6036-4717- 주관심분야: 텍스트마이닝, 기계학습, 딥러닝김학동- 2016년: 경성대학교컴퓨터공학과학사- 2017년~ 현재: 세종대학교디지털콘텐츠학과석,박사통합과정- ORCID : https://orcid.org/0000-0003-3816-1224- 주관심분야: 머신러닝, 딥러닝, 자연어처리임헌영- 2017년: 세종대학교디지털콘텐츠학과학사- 2017년~ 현재: 세종대학교디지털콘텐츠학과석사과정- ORCID : https://orcid.org/0000-0002-8547-6248- 주관심분야: 컴퓨터비전, 기계학습, 딥러닝\fwww.dbpia.co.kr\n",
      "\n",
      "86방송공학회논문지제24권제1호,2019년1월(JBE Vol. 24, No. 1, January 2019)저자소개이유림- 2018년: 세종대학교디지털콘텐츠학과학사- 2018년~ 현재: 세종대학교인공지능언어공학과석사과정- ORCID : https://orcid.org/0000-0001-8309-090X- 주관심분야: 텍스트마이닝, 자연어처리, 딥러닝김원일- 1981년12월~ 1985년7월: ㈜대한항공전산실재무시스템개발원- 1982년: 한양대학교공과대학금속공학학사- 1987년: 미국일리노이주서던일리노이대학교컴퓨터과학학사- 1990년: 미국일리노이주서던일리노이대학교컴퓨터과학석사- 1994년: 미국인디아나주인디애나대학교대학원컴퓨터과학전공- 2000년: 미국뉴욕주시러큐스대학교대학원컴퓨터& 정보과학공학박사- 2000년1월~ 2001년3월: 미국펜실바니아주외인시소재Bhasha, INC Technical Staff (연구원)  - 2002년3월~ 2003년8월: 아주대학교정보통신전문대학원BK 교수- 2003년9월~ 2017년2월: 세종대학교전자정보공학대학교수- 2017년3월~ 현재: 세종대학교소프트웨어융합대학교수- ORCID : https://orcid.org/0000-0002-1489-8427- 주관심분야: 인공지능, 지능형시스템, 딥러닝등\f\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "text = extract_text(\"test3.pdf\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "서\n",
      "론\n",
      "분\n",
      "석\n",
      "배\n",
      "경\n",
      "및\n",
      "목\n",
      "적\n",
      "서\n",
      "울\n",
      "시\n",
      "안\n",
      "심\n",
      "홈\n",
      "세\n",
      "트\n",
      "지\n",
      "원\n",
      "서\n",
      "비\n",
      "스\n",
      "정\n",
      "책\n",
      "분\n",
      "석\n",
      "2019\n",
      "년부터\n",
      "전국\n",
      "지자체에서\n",
      "여성\n",
      "1\n",
      "인가구를\n",
      "대상으로\n",
      "한\n",
      "안심지원사업을\n",
      "시범적으로\n",
      "운영하기\n",
      "시작했고\n",
      ",\n",
      "2021\n",
      "년\n",
      "6\n",
      "월부터\n",
      "서울시에서는\n",
      "사업을\n",
      "더\n",
      "많은\n",
      "자치구로\n",
      "확대했다\n",
      ".\n",
      "정책에\n",
      "대한\n",
      "만족도는\n",
      "높았으나\n",
      ",\n",
      "여성\n",
      "1\n",
      "인가구\n",
      "수에\n",
      "비해\n",
      "지원은\n",
      "턱없이\n",
      "부족했다\n",
      ".\n",
      "이는\n",
      "시범운영이\n",
      "이루어지던\n",
      "시기였기에\n",
      "차차\n",
      "지원범위를\n",
      "확대해나가야\n",
      "한다\n",
      ".\n",
      "따라서\n",
      "본\n",
      "연구자들은\n",
      "서울시\n",
      "안심홈세트\n",
      "지원정책을\n",
      "활성화하기\n",
      "위해\n",
      "R,\n",
      "Python\n",
      "등을\n",
      "이용해\n",
      "서울시의\n",
      "행정구별\n",
      "여러\n",
      "변수를\n",
      "분석하고\n",
      ",\n",
      "시각화하여\n",
      "지자체에서\n",
      "‘\n",
      "우선지원분배\n",
      "행정구\n",
      "선택방법\n",
      "’\n",
      "을\n",
      "제안하고자\n",
      "한다\n",
      ".\n",
      "여\n",
      "성\n",
      "1\n",
      "인\n",
      "가\n",
      "구\n",
      "에\n",
      "대\n",
      "한\n",
      "정\n",
      "책\n",
      "지\n",
      "원\n",
      "미\n",
      "비\n",
      "1\n",
      "인가구\n",
      "밀집지역은\n",
      "비밀집지역에\n",
      "비해\n",
      "2\n",
      "~\n",
      "3\n",
      "배\n",
      "높은\n",
      "범죄발생율\n",
      "을\n",
      "보인다\n",
      ".\n",
      "또한\n",
      "주요\n",
      "5\n",
      "대\n",
      "강력범죄가\n",
      "주거지에서\n",
      "가장\n",
      "많이\n",
      "발생하며\n",
      ",\n",
      "여성\n",
      "피해자의\n",
      "수도\n",
      "꾸준히\n",
      "증가세를\n",
      "보이고\n",
      "있다\n",
      ".\n",
      "빅카인즈를\n",
      "통해\n",
      "2019\n",
      "년\n",
      "5\n",
      "월\n",
      "‘\n",
      "신림동\n",
      "강간미수\n",
      "사건\n",
      "’\n",
      "전후로\n",
      "’\n",
      "1\n",
      "인가구\n",
      "and\n",
      "여성\n",
      "’\n",
      "키워드로\n",
      "연관어\n",
      "분석을\n",
      "해보았을\n",
      "때\n",
      ",\n",
      "주거공간에서의\n",
      "범죄불안\n",
      "및\n",
      "지원정책에\n",
      "대한\n",
      "수요가\n",
      "증가했음을\n",
      "알\n",
      "수\n",
      "있었다\n",
      ".\n",
      "따라서\n",
      "본\n",
      "연구에서는\n",
      "정책지원\n",
      "필요성이\n",
      "크게\n",
      "증가했음을\n",
      "인지하고\n",
      ",\n",
      "이에\n",
      "대해\n",
      "구체적인\n",
      "지원이\n",
      "이루어져야\n",
      "하기에\n",
      "여성\n",
      "1\n",
      "인가구에\n",
      "대한\n",
      "정책을\n",
      "찾아\n",
      "분석했다\n",
      ".\n",
      "1\n",
      "인\n",
      "가\n",
      "구\n",
      "의\n",
      "증\n",
      "가\n",
      "로\n",
      "인\n",
      "한\n",
      "연\n",
      "구\n",
      "필\n",
      "요\n",
      "성\n",
      "대\n",
      "두\n",
      "서울시의\n",
      "1\n",
      "인가구\n",
      "수는\n",
      "2000\n",
      "년\n",
      "50\n",
      "만명에서부터\n",
      "2015\n",
      "년\n",
      "100\n",
      "만명을\n",
      "넘기고\n",
      ",\n",
      "2019\n",
      "년에는\n",
      "약\n",
      "130\n",
      "만명에\n",
      "이를만큼\n",
      "꾸준히\n",
      "증가하고\n",
      "있다\n",
      ".\n",
      "또한\n",
      "주된\n",
      "가구\n",
      "구성방식이\n",
      "핵가족\n",
      "단위에서\n",
      "1\n",
      "인가구\n",
      "단위로\n",
      "바뀌어가고\n",
      "있다\n",
      ".\n",
      "학계에서도\n",
      "많은\n",
      "연구가\n",
      "이루어지고\n",
      "있기에\n",
      "지자체에서도\n",
      "이에\n",
      "주목하고\n",
      ",\n",
      "구체적\n",
      "정책을\n",
      "제시해야\n",
      "한다\n",
      ".\n",
      "서울특별시\n",
      "1\n",
      "인가구\n",
      "대책정책\n",
      "연구\n",
      "(\n",
      "서울시\n",
      ",\n",
      "2014\n",
      ")\n",
      "에서는\n",
      "1\n",
      "인가구\n",
      "정책을\n",
      "’\n",
      "주거주택\n",
      "부문\n",
      "’,\n",
      "‘\n",
      "사회적안전망\n",
      "부문\n",
      "’,\n",
      "‘\n",
      "공유사회\n",
      "부문\n",
      "’\n",
      "세\n",
      "가지로\n",
      "나누고\n",
      "있는데\n",
      ",\n",
      "본\n",
      "연구에서는\n",
      "‘\n",
      "사회적안전망\n",
      "부문\n",
      "’\n",
      "에\n",
      "주목해\n",
      "보았다\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf = open('test.pdf', 'rb')\n",
    "reader = PyPDF2.PdfReader(pdf)\n",
    "page = reader.pages[2]\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppt_3_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0cc08dc29aad09da65be7821d0eeaddf64caec4e877dc2ce9e5b3cd8bcad43f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
